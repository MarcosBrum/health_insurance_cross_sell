{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.342347Z",
     "start_time": "2021-02-26T17:35:53.416524Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit as sss\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.345863Z",
     "start_time": "2021-02-26T17:35:54.343596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "home_path = \"/home/marcos/Documentos/comunidade_DS/pa004_health_insurance_cross_sell/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.369629Z",
     "start_time": "2021-02-26T17:35:54.347574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_tree_train = pd.read_pickle(home_path + \"interim/df6_bal_tree_train.pkl\")\n",
    "\n",
    "df_tree_validation = pd.read_pickle(home_path + \"interim/df6_bal_tree_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.2 Separate train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.382847Z",
     "start_time": "2021-02-26T17:35:54.370941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df7_train = df_tree_train.copy()\n",
    "\n",
    "df7_validation = df_tree_validation.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.396379Z",
     "start_time": "2021-02-26T17:35:54.384453Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vintage                 0\n",
       "annual_premium          0\n",
       "age                     0\n",
       "region_code             0\n",
       "policy_sales_channel    0\n",
       "vehicle_hist            0\n",
       "vehicle_damage          0\n",
       "previously_insured      0\n",
       "weight_ages             0\n",
       "id                      0\n",
       "response                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.405272Z",
     "start_time": "2021-02-26T17:35:54.397950Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vintage                 0\n",
       "annual_premium          0\n",
       "age                     0\n",
       "region_code             0\n",
       "policy_sales_channel    3\n",
       "vehicle_hist            0\n",
       "vehicle_damage          0\n",
       "previously_insured      0\n",
       "weight_ages             0\n",
       "id                      0\n",
       "response                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_validation.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.410535Z",
     "start_time": "2021-02-26T17:35:54.406980Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76222, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I will drop the rows containing NAs from df7_validation. Since it contains over 76k rows, this must not make a big difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.432078Z",
     "start_time": "2021-02-26T17:35:54.413649Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vintage                 0\n",
       "annual_premium          0\n",
       "age                     0\n",
       "region_code             0\n",
       "policy_sales_channel    0\n",
       "vehicle_hist            0\n",
       "vehicle_damage          0\n",
       "previously_insured      0\n",
       "weight_ages             0\n",
       "id                      0\n",
       "response                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_validation.dropna(axis=0, inplace=True)\n",
    "df7_validation.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.437281Z",
     "start_time": "2021-02-26T17:35:54.434135Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76219, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.3 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:35:54.452050Z",
     "start_time": "2021-02-26T17:35:54.439057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# precision_at_k\n",
    "def precision_at_k(data, k):\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['n_samples'] = data.index + 1\n",
    "    data['precision_at_k'] = data['response'].cumsum() / data['n_samples']\n",
    "    return data.loc[k, 'precision_at_k']\n",
    "\n",
    "# recall_at_k\n",
    "def recall_at_k(data, k):\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['recall_at_k'] = data['response'].cumsum() / data['response'].sum()\n",
    "    return data.loc[k, 'recall_at_k']\n",
    "\n",
    "# model predict\n",
    "def model_evaluate(model, model_name, data_train, data_val, k):\n",
    "    # separate X and Y data:\n",
    "    xtrain = data_train.drop(['id', 'response'], axis=1)\n",
    "    ytrain = data_train.response\n",
    "    xval = data_val.drop(['id', 'response'], axis=1)\n",
    "    yval = data_val.response\n",
    "    \n",
    "    # fit and predict_proba:\n",
    "    model.fit(xtrain, ytrain)\n",
    "    yhat_proba = model.predict_proba(xval)\n",
    "    \n",
    "    # transform yhat_proba to 1D-array\n",
    "    yhat_proba_1d = yhat_proba[:, 1].tolist()\n",
    "    \n",
    "    # include in dataframe\n",
    "    validation_data = data_val.copy()\n",
    "    validation_data['score'] = yhat_proba_1d\n",
    "    # sort\n",
    "    validation_data = validation_data.sort_values('score', ascending=False)\n",
    "    \n",
    "    # plot\n",
    "    skplt.metrics.plot_cumulative_gain(yval, yhat_proba);\n",
    "    \n",
    "    return pd.DataFrame({'Model name':model_name,\n",
    "                         'precision_at_k':precision_at_k(validation_data, k),\n",
    "                         'recall_at_k':recall_at_k(validation_data, k)}, index=[0])\n",
    "\n",
    "# model fit\n",
    "def model_fit(model, data):\n",
    "    # separate X and Y data:\n",
    "    xtrain = data.drop(['id', 'response'], axis=1)\n",
    "    ytrain = data.response\n",
    "    \n",
    "    # fit\n",
    "    model_fitted = model.fit(xtrain, ytrain)\n",
    "    \n",
    "    return model_fitted\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation(model, model_name, training_data, k_top, kfolds, verbose=False):\n",
    "    # X separate X and Y data:\n",
    "    xtraining = training_data.drop(['response'], axis=1)\n",
    "    ytraining = training_data.response\n",
    "    \n",
    "    # cross-validation:\n",
    "    cv = sss(n_splits=kfolds)\n",
    "    prec_k_list = []\n",
    "    rec_k_list = []\n",
    "    for train_index, prim_val_index in cv.split(xtraining, ytraining):\n",
    "        X_training, X_prim_val = xtraining.iloc[train_index], xtraining.iloc[prim_val_index]\n",
    "        y_training, y_prim_val = ytraining.iloc[train_index], ytraining.iloc[prim_val_index]\n",
    "        \n",
    "        # remove id from training, and create new validation without id\n",
    "        X_training = X_training.drop(['id'], axis=1)\n",
    "        X_prim_val_no_id = X_prim_val.drop(['id'], axis=1)\n",
    "        \n",
    "        # fit and predict_proba\n",
    "        model.fit(X_training, y_training)\n",
    "        yhat_proba = model.predict_proba(X_prim_val_no_id)\n",
    "        \n",
    "        # transform yhat_proba to 1D-array\n",
    "        yhat_proba_1d = yhat_proba[:, 1].tolist()\n",
    "        \n",
    "        # reconstruct dataframe\n",
    "        prim_val = pd.concat([X_prim_val, y_prim_val], axis=1)\n",
    "        prim_val['score'] = yhat_proba_1d\n",
    "        prim_val = prim_val.sort_values('score', ascending=False)\n",
    "        \n",
    "        # evaluate accuracy and store in list\n",
    "        prec_k_list.append(precision_at_k(prim_val, k_top))\n",
    "        rec_k_list.append(recall_at_k(prim_val, k_top))\n",
    "    \n",
    "    #evaluate mean and std\n",
    "    prec_k_pred = np.round(np.mean(prec_k_list), 4).astype(str) + '+/-' + np.round(np.std(prec_k_list), 4).astype(str)\n",
    "    rec_k_pred = np.round(np.mean(rec_k_list), 4).astype(str) + '+/-' + np.round(np.std(rec_k_list), 4).astype(str)\n",
    "    \n",
    "    return pd.DataFrame({'Model name':model_name,\n",
    "                         'precision_at_k':prec_k_pred,\n",
    "                         'recall_at_k':rec_k_pred}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 7.0 Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.1 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7.1.1 Easy Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T18:10:30.231781Z",
     "start_time": "2021-02-26T17:35:54.453631Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>precision_at_k</th>\n",
       "      <th>recall_at_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy Ensemble Classifier</td>\n",
       "      <td>0.1867+/-0.0001</td>\n",
       "      <td>0.9993+/-0.0003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model name   precision_at_k      recall_at_k\n",
       "0  Easy Ensemble Classifier  0.1867+/-0.0001  0.9993+/-0.0003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_ens = EasyEnsembleClassifier(n_estimators=150, sampling_strategy=0.15,\n",
    "                                  replacement=True, n_jobs=-1, random_state=50)\n",
    "\n",
    "e_ens_cv_metrics = cross_validation(easy_ens, \"Easy Ensemble Classifier\", df7_train, 20000, 5)\n",
    "\n",
    "e_ens_cv_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
